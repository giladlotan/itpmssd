{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "# Twitter API keys go here\n",
    "CONSUMER_KEY = ''\n",
    "CONSUMER_SECRET = ''\n",
    "\n",
    "OAUTH_TOKEN = ''\n",
    "OAUTH_TOKEN_SECRET = ''\n",
    "\n",
    "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "twitter_api = twitter.Twitter(auth=auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = '#FirstWorldProblems' \n",
    "count = 100\n",
    "\n",
    "tweet_texts = []\n",
    "tweet_mentions = []\n",
    "tweet_hashtags = []\n",
    "tweet_bios = []\n",
    "ids = []\n",
    "\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# save relevant data\n",
    "ids += [status['id'] for status in statuses]\n",
    "tweet_texts += [status['text'] for status in statuses]\n",
    "tweet_mentions += [mention['screen_name'] for status in statuses\n",
    "                       for mention in status['entities']['user_mentions']]\n",
    "tweet_hashtags += [hashtag['text'].lower() for status in statuses\n",
    "                       for hashtag in status['entities']['hashtags']]\n",
    "tweet_bios += [status['user']['description'] for status in statuses \n",
    "                       if status['user']['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n"
     ]
    }
   ],
   "source": [
    "# use a loop\n",
    "num_iterations = 20\n",
    "print len(set(ids))\n",
    "for i in range(num_iterations):\n",
    "    params = {a:b for a,b in [x.split('=') for x in search_results['search_metadata']['next_results'][1:].split('&')]}\n",
    "    max_id = int(params['max_id'])\n",
    "    search_results = twitter_api.search.tweets(q=q, count=count, max_id=max_id)\n",
    "    statuses = search_results['statuses']\n",
    "    \n",
    "    # save relevant data\n",
    "    tweet_texts += [status['text'] for status in statuses]\n",
    "    tweet_mentions += [mention['screen_name'] for status in statuses\n",
    "                           for mention in status['entities']['user_mentions']]\n",
    "    tweet_hashtags += [hashtag['text'].lower() for status in statuses\n",
    "                           for hashtag in status['entities']['hashtags']]\n",
    "    tweet_bios += [status['user']['description'] for status in statuses \n",
    "                           if status['user']['description']]\n",
    "    \n",
    "    ids += [status['id'] for status in statuses]\n",
    "    print len(set(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'limit': 180, u'remaining': 138, u'reset': 1446214191}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many more calls do we have?\n",
    "twitter_api.application.rate_limit_status()['resources']['search']['/search/tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1949"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only represents a few different tweets\n",
    "len(set(tweet_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "arr = [1,2,3,4,5,6,7,1,2,3,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = Counter(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 2), (3, 2), (7, 2), (4, 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'I love having my nails painted but I hate painting my nails and getting them done is expensive... #firstworldproblems'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_texts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'RT @ItsSeppee_: NEE NI STOPPEN NEE WTF WA IS ER MET DIE OMA EN HOE LOOPT HET AF MET JAN EN LEYLA?? WRM AL DIE CLIFFHANGERS #D5R #firstworld\\u2026',\n",
       "  70),\n",
       " (u'RT @holapun: \\xbfUsted cree que una quesadilla sin queso es quesadilla? #firstworldproblems',\n",
       "  14),\n",
       " (u\"RT @THBreak: When you're hungry but nothing sounds good \\U0001f629\\n\\n#firstworldproblems\",\n",
       "  11),\n",
       " (u'#firstworldproblems', 9),\n",
       " (u'#FirstWorldProblems', 5),\n",
       " (u'RT @MsJaimeMurray: @juliebenz Oh yes... if they make it small enough &amp; put it by the cash register I am guaranteed to buy it! #Sephora #fir\\u2026',\n",
       "  5),\n",
       " (u'RT @dielochis: Hatten dieses Jahr schon 100+ Fahrten mit @mytaxi &amp; immer noch nicht den VIP Status erreicht, seltsam. Haha. #firstworldprob\\u2026',\n",
       "  5),\n",
       " (u\"RT @Cudlitz: 'night, y'all...... 'nother long day. 'nother early mornin' #FirstWorldProblems \\U0001f44a\\U0001f44a\",\n",
       "  5),\n",
       " (u\"RT @timjn1: Only one #FF @SoMuchGuardian - deliciously mocking the world's most privileged ninnies and their #firstworldproblems.\",\n",
       "  4),\n",
       " (u'RT @chloedentonxo: Waiting for a parcel to be delivered is like watching paint dry #FirstWorldProblems  \\U0001f614',\n",
       "  4)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tweet_texts).most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Why',\n",
       " u'is',\n",
       " u'it',\n",
       " u'so',\n",
       " u'hard',\n",
       " u'to',\n",
       " u'comprehend',\n",
       " u'a',\n",
       " u'tandem',\n",
       " u'drive-thru?',\n",
       " u'#HowDidWeEndUpHere',\n",
       " u'#McDonalds',\n",
       " u'#genious',\n",
       " u'#useyourbrain',\n",
       " u'#firstworldproblems']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_texts[1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " \"@c_teall\", \n",
      " \"#firstworldproblems\", \n",
      " \"Why\", \n",
      " \"is\", \n",
      " \"it\", \n",
      " \"so\", \n",
      " \"hard\", \n",
      " \"to\", \n",
      " \"comprehend\", \n",
      " \"a\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [w for tweet_text in tweet_texts for w in tweet_text.split()]\n",
    "\n",
    "print json.dumps(words[0:10], indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bio_words = [w for bio in tweet_bios for w in bio.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Gin',\n",
       " u'lover,',\n",
       " u'sweet',\n",
       " u'lover',\n",
       " u'and',\n",
       " u'managing',\n",
       " u'editor',\n",
       " u'at',\n",
       " u'@cogentoa,',\n",
       " u'the']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 549),\n",
       " (u'the', 421),\n",
       " (u'of', 342),\n",
       " (u'a', 338),\n",
       " (u'I', 329),\n",
       " (u'to', 280),\n",
       " (u'|', 256),\n",
       " (u'&', 248),\n",
       " (u'in', 235),\n",
       " (u'my', 218)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(bio_words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'#firstworldproblems', 1693), (u'to', 660), (u'I', 636), (u'the', 566), (u'a', 482), (u'my', 414), (u'RT', 292), (u'and', 284), (u'is', 273), (u'in', 266)]\n",
      "\n",
      "[(u'and', 549), (u'the', 421), (u'of', 342), (u'a', 338), (u'I', 329), (u'to', 280), (u'|', 256), (u'&', 248), (u'in', 235), (u'my', 218)]\n",
      "\n",
      "[(u'ItsSeppee_', 70), (u'holapun', 14), (u'THBreak', 11), (u'Starbucks', 8), (u'firstworldme', 7), (u'juliebenz', 6), (u'SoMuchGuardian', 5), (u'MsJaimeMurray', 5), (u'mytaxi', 5), (u'Tesco', 5)]\n",
      "\n",
      "[(u'firstworldproblems', 2100), (u'd5r', 71), (u'halloween', 10), (u'thestruggleisreal', 8), (u'decisions', 7), (u'sephora', 6), (u'iphone', 5), (u'ff', 5), (u'nationalcatday', 5), (u'wfsoc', 4)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print top items from each array\n",
    "from collections import Counter\n",
    "\n",
    "for item in [words, bio_words, tweet_mentions, tweet_hashtags]:\n",
    "    c = Counter(item)\n",
    "    print c.most_common()[:10] # top 10\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Regular Expression\n",
    "import re\n",
    "\n",
    "stub = re.compile('[^A-Za-z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @ItsSeppee_: NEE NI STOPPEN NEE WTF WA IS ER MET DIE OMA EN HOE LOOPT HET AF MET JAN EN LEYLA?? WRM AL DIE CLIFFHANGERS #D5R #firstworldâ€¦\n",
      "RT @holapun: Â¿Usted cree que una quesadilla sin queso es quesadilla? #firstworldproblems\n",
      "RT @THBreak: When you're hungry but nothing sounds good ðŸ˜©\n",
      "\n",
      "#firstworldproblems\n",
      "#firstworldproblems\n",
      "#FirstWorldProblems\n",
      "RT @MsJaimeMurray: @juliebenz Oh yes... if they make it small enough &amp; put it by the cash register I am guaranteed to buy it! #Sephora #firâ€¦\n",
      "RT @dielochis: Hatten dieses Jahr schon 100+ Fahrten mit @mytaxi &amp; immer noch nicht den VIP Status erreicht, seltsam. Haha. #firstworldprobâ€¦\n",
      "RT @timjn1: Only one #FF @SoMuchGuardian - deliciously mocking the world's most privileged ninnies and their #firstworldproblems.\n",
      "RT @chloedentonxo: Waiting for a parcel to be delivered is like watching paint dry #FirstWorldProblems  ðŸ˜”\n",
      "RT @Cudlitz: 'night, y'all...... 'nother long day. 'nother early mornin' #FirstWorldProblems ðŸ‘ŠðŸ‘Š\n"
     ]
    }
   ],
   "source": [
    "c = Counter(tweet_texts)\n",
    "\n",
    "for tw, v in c.most_common(10):\n",
    "    print tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'RT @ItsSeppee_: NEE NI STOPPEN NEE WTF WA IS ER MET DIE OMA EN HOE LOOPT HET AF MET JAN EN LEYLA?? WRM AL DIE CLIFFHANGERS #D5R #firstworld\\u2026'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw = c.most_common(1)\n",
    "tw[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_tweet = []\n",
    "for w in tw[0][0].split():\n",
    "    new_tweet.append(stub.sub('',w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'RT ItsSeppee NEE NI STOPPEN NEE WTF WA IS ER MET DIE OMA EN HOE LOOPT HET AF MET JAN EN LEYLA WRM AL DIE CLIFFHANGERS DR firstworld'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(new_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT ItsSeppee NEE NI STOPPEN NEE WTF WA IS ER MET DIE OMA EN HOE LOOPT HET AF MET JAN EN LEYLA WRM AL DIE CLIFFHANGERS DR firstworld\n",
      "RT holapun Usted cree que una quesadilla sin queso es quesadilla firstworldproblems\n",
      "RT THBreak When youre hungry but nothing sounds good  firstworldproblems\n",
      "firstworldproblems\n",
      "FirstWorldProblems\n",
      "RT MsJaimeMurray juliebenz Oh yes if they make it small enough amp put it by the cash register I am guaranteed to buy it Sephora fir\n",
      "RT dielochis Hatten dieses Jahr schon  Fahrten mit mytaxi amp immer noch nicht den VIP Status erreicht seltsam Haha firstworldprob\n",
      "RT timjn Only one FF SoMuchGuardian  deliciously mocking the worlds most privileged ninnies and their firstworldproblems\n",
      "RT chloedentonxo Waiting for a parcel to be delivered is like watching paint dry FirstWorldProblems \n",
      "RT Cudlitz night yall nother long day nother early mornin FirstWorldProblems \n"
     ]
    }
   ],
   "source": [
    "# length <=2 - unimportant?\n",
    "for tw, v in c.most_common(10):\n",
    "    print ' '.join([stub.sub('',w) for w in tw.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get rid of words that start with '@'\n",
    "\n",
    "for tw, v in c.most_common(2):\n",
    "    print ' '.join([stub.sub('',w) for w in tw.split() if not w.startswith('@')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEE STOPPEN NEE WTF MET DIE OMA HOE LOOPT HET MET JAN LEYLA WRM DIE CLIFFHANGERS firstworld\n",
      "Usted cree quesadilla sin queso quesadilla firstworldproblems\n",
      "When youre hungry but nothing sounds good firstworldproblems\n",
      "firstworldproblems\n",
      "FirstWorldProblems\n",
      "yes they make small enough amp put cash register guaranteed buy Sephora fir\n",
      "Hatten dieses Jahr schon Fahrten mit amp immer noch nicht den VIP Status erreicht seltsam Haha firstworldprob\n",
      "Only one deliciously mocking worlds most privileged ninnies and their firstworldproblems\n",
      "Waiting parcel delivered like watching paint dry FirstWorldProblems\n",
      "night yall nother long day nother early mornin FirstWorldProblems\n"
     ]
    }
   ],
   "source": [
    "blacklist = ['i','am','for','can','via','with','top','las','una','que','hay','todo','por','the','going','you','are','not','its','until']\n",
    "\n",
    "for tw, v in c.most_common(10):\n",
    "    cur_tweet = []\n",
    "    for w in tw.split():\n",
    "        if w.startswith('@'):\n",
    "            continue\n",
    "            \n",
    "        cur_w = stub.sub('', w)\n",
    "        if len(cur_w)<3:\n",
    "            continue\n",
    "            \n",
    "        if cur_w.lower() in blacklist:\n",
    "            continue\n",
    "            \n",
    "        cur_tweet.append(cur_w)\n",
    "        \n",
    "    print ' '.join(cur_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> stopwords\n",
      "    Downloading package stopwords to /root/nltk_data...\n",
      "      Package stopwords is already up-to-date!\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install nltk\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves',\n",
       " u'he',\n",
       " u'him',\n",
       " u'his',\n",
       " u'himself',\n",
       " u'she',\n",
       " u'her',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'they',\n",
       " u'them',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'themselves',\n",
       " u'what',\n",
       " u'which',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'this',\n",
       " u'that',\n",
       " u'these',\n",
       " u'those',\n",
       " u'am',\n",
       " u'is',\n",
       " u'are',\n",
       " u'was',\n",
       " u'were',\n",
       " u'be',\n",
       " u'been',\n",
       " u'being',\n",
       " u'have',\n",
       " u'has',\n",
       " u'had',\n",
       " u'having',\n",
       " u'do',\n",
       " u'does',\n",
       " u'did',\n",
       " u'doing',\n",
       " u'a',\n",
       " u'an',\n",
       " u'the',\n",
       " u'and',\n",
       " u'but',\n",
       " u'if',\n",
       " u'or',\n",
       " u'because',\n",
       " u'as',\n",
       " u'until',\n",
       " u'while',\n",
       " u'of',\n",
       " u'at',\n",
       " u'by',\n",
       " u'for',\n",
       " u'with',\n",
       " u'about',\n",
       " u'against',\n",
       " u'between',\n",
       " u'into',\n",
       " u'through',\n",
       " u'during',\n",
       " u'before',\n",
       " u'after',\n",
       " u'above',\n",
       " u'below',\n",
       " u'to',\n",
       " u'from',\n",
       " u'up',\n",
       " u'down',\n",
       " u'in',\n",
       " u'out',\n",
       " u'on',\n",
       " u'off',\n",
       " u'over',\n",
       " u'under',\n",
       " u'again',\n",
       " u'further',\n",
       " u'then',\n",
       " u'once',\n",
       " u'here',\n",
       " u'there',\n",
       " u'when',\n",
       " u'where',\n",
       " u'why',\n",
       " u'how',\n",
       " u'all',\n",
       " u'any',\n",
       " u'both',\n",
       " u'each',\n",
       " u'few',\n",
       " u'more',\n",
       " u'most',\n",
       " u'other',\n",
       " u'some',\n",
       " u'such',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'only',\n",
       " u'own',\n",
       " u'same',\n",
       " u'so',\n",
       " u'than',\n",
       " u'too',\n",
       " u'very',\n",
       " u's',\n",
       " u't',\n",
       " u'can',\n",
       " u'will',\n",
       " u'just',\n",
       " u'don',\n",
       " u'should',\n",
       " u'now']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get this working I've had to restart my ipython notebook server (which is a bit annoying -> \n",
    "# but we can do it together in class)\n",
    "# easiest is to simply restart our machines\n",
    "\n",
    "# Here's the command: \n",
    "#          /usr/bin/python /usr/local/bin/ipython notebook --notebook-dir=/class --no-browser --port=8887 &\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEE\n",
      "STOPPEN\n",
      "NEE\n",
      "WTF\n",
      "MET\n",
      "DIE\n",
      "OMA\n",
      "HOE\n",
      "LOOPT\n",
      "HET\n",
      "MET\n",
      "JAN\n",
      "LEYLA\n",
      "WRM\n",
      "DIE\n",
      "CLIFFHANGERS\n",
      "firstworld\n",
      "Usted\n",
      "cree\n",
      "quesadilla\n",
      "queso\n",
      "quesadilla\n",
      "firstworldproblems\n",
      "youre\n",
      "hungry\n",
      "nothing\n",
      "sounds\n",
      "good\n",
      "firstworldproblems\n",
      "firstworldproblems\n",
      "FirstWorldProblems\n",
      "yes\n",
      "make\n",
      "small\n",
      "enough\n",
      "amp\n",
      "put\n",
      "cash\n",
      "register\n",
      "guaranteed\n",
      "buy\n",
      "Sephora\n",
      "fir\n",
      "Hatten\n",
      "dieses\n",
      "Jahr\n",
      "schon\n",
      "Fahrten\n",
      "mit\n",
      "amp\n",
      "immer\n",
      "noch\n",
      "nicht\n",
      "den\n",
      "VIP\n",
      "Status\n",
      "erreicht\n",
      "seltsam\n",
      "Haha\n",
      "firstworldprob\n",
      "one\n",
      "deliciously\n",
      "mocking\n",
      "worlds\n",
      "privileged\n",
      "ninnies\n",
      "firstworldproblems\n",
      "Waiting\n",
      "parcel\n",
      "delivered\n",
      "like\n",
      "watching\n",
      "paint\n",
      "dry\n",
      "FirstWorldProblems\n",
      "night\n",
      "yall\n",
      "nother\n",
      "long\n",
      "day\n",
      "nother\n",
      "early\n",
      "mornin\n",
      "FirstWorldProblems\n"
     ]
    }
   ],
   "source": [
    "for tw, v in c.most_common(10):\n",
    "    for w in tw.split():\n",
    "        if w.startswith('@'):\n",
    "            continue\n",
    "        cur_w = stub.sub('', w)\n",
    "        if len(cur_w)<3:\n",
    "            continue\n",
    "        if cur_w.lower() in stopwords.words('english'):\n",
    "            continue\n",
    "        if cur_w.lower() in stopwords.words('spanish'):\n",
    "            continue\n",
    "            \n",
    "        print cur_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# common words\n",
    "common_words = ['yes','way','youre']\n",
    "\n",
    "for tw, v in c.most_common(10):\n",
    "    for w in tw.split():\n",
    "        if w.startswith('http://'):\n",
    "            continue\n",
    "        cur_w = stub.sub('', w)\n",
    "        if len(cur_w)<3:\n",
    "            continue\n",
    "        if cur_w.lower() in stopwords.words('english'):\n",
    "            continue\n",
    "        if cur_w.lower() in stopwords.words('spanish'):\n",
    "            continue\n",
    "        if cur_w.lower() in common_words:\n",
    "            continue\n",
    "            \n",
    "        print cur_w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Hashtags, Mentions using a regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hashtags\n",
    "import re\n",
    "\n",
    "re_hash = re.compile(r'#[0-9a-zA-Z+_]*',re.IGNORECASE)\n",
    "hashtags = Counter()\n",
    "\n",
    "for tw in c.keys():\n",
    "    hashtags.update(re.findall(re_hash, tw))\n",
    "\n",
    "# ---> other ways to do this:\n",
    "# print re.findall(r'#(\\w+)', tw)\n",
    "# print re.findall(r'\\B#\\w+', tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtags.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mentions\n",
    "re_mention = re.compile(r'@[0-9a-zA-Z+_]*',re.IGNORECASE)\n",
    "mentions = Counter()\n",
    "\n",
    "for tw in c.keys():\n",
    "    mentions.update(re.findall(re_mention, tw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mentions.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Capitalized Phrases using a regular expression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A Counter is a container that keeps track of how many times equivalent values are added. Good tutorial here - http://pymotw.com/2/collections/counter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "phrases = Counter()\n",
    "\n",
    "comp = re.compile(r'([A-Z][a-z]+(?=\\s[A-Z])(?:\\s[A-Z][a-z]+)+)')\n",
    "for tw, v in c.items():\n",
    "    phrases.update(re.findall(comp, tw))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(                # begin capture\n",
    "  [A-Z]            # one uppercase letter  \\ First Word\n",
    "  [a-z]+           # 1+ lowercase letters  /\n",
    "  (?=\\s[A-Z])      # must have a space and uppercase letter following it\n",
    "  (?:                # non-capturing group\n",
    "    \\s               # space\n",
    "    [A-Z]            # uppercase letter   \\ Additional Word(s)\n",
    "    [a-z]+           # lowercase letter   /\n",
    "  )+              # group can be repeated (more words)\n",
    ")               #end capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Sri Lanka', 4),\n",
       " (u'Instagram Feed', 3),\n",
       " (u'Rite Aid', 2),\n",
       " (u'Problems Spoon Fed Pricks', 2),\n",
       " (u'Bei Lieferando', 2),\n",
       " (u'Apple Watch', 2),\n",
       " (u'No Kittens Available', 2),\n",
       " (u'Southampton Town', 2),\n",
       " (u'Ordered Indian', 2),\n",
       " (u'British Irony', 2)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a bi-partite graph using top phrases, hashtags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "limit = 5000\n",
    "\n",
    "top_phrases = [x for x,y in phrases.most_common(limit)]\n",
    "top_hashtags = [x for x,y in hashtags.most_common(limit) if len(x)>2]\n",
    "top_mentions = [x for x,y in mentions.most_common(limit) if len(x)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_hashtags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_mentions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# functions that help us construct the graph\n",
    "def graph_add_node(n, g, node_type=None):\n",
    "    try:\n",
    "        if g.has_node(n):\n",
    "            g.node[n]['weight']+=1\n",
    "        else:\n",
    "            g.add_node(n)\n",
    "            g.node[n]['label'] = n\n",
    "            g.node[n]['weight'] = 1\n",
    "            g.node[n]['type']=node_type\n",
    "    except:\n",
    "        return\n",
    "            \n",
    "def graph_add_edge(n1, n2, g):\n",
    "    if g.has_edge(n1, n2):\n",
    "        g[n1][n2]['weight']+=1\n",
    "    else:\n",
    "        g.add_edge(n1,n2)\n",
    "        g[n1][n2]['weight']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get item co-occurrence\n",
    "from itertools import combinations\n",
    "\n",
    "stub = re.compile('[^A-Za-z#@]')\n",
    "re_phrase = re.compile(r'([A-Z][a-z]+(?=\\s[A-Z])(?:\\s[A-Z][a-z]+)+)')\n",
    "re_mention_hashtag = re.compile(r'[@|#][0-9a-zA-Z+_]*',re.IGNORECASE)\n",
    "\n",
    "g = nx.Graph()\n",
    "\n",
    "for t,v in c.most_common(30000):\n",
    "    cur_items = set()\n",
    "    \n",
    "    for phrase in re.findall(re_phrase, t):\n",
    "        cur_items.add(phrase)\n",
    "        graph_add_node(phrase, g, node_type='phrase')\n",
    "        \n",
    "    for men_hash in re.findall(re_mention_hashtag, t):\n",
    "        cur_items.add(men_hash)\n",
    "        if men_hash.startswith('#'):\n",
    "            graph_add_node(men_hash, g, node_type='hashtag')\n",
    "        elif men_hash.startswith('@'):\n",
    "            graph_add_node(men_hash, g, node_type='mention')\n",
    "        \n",
    "    # add edges\n",
    "    for i1, i2 in combinations(cur_items, 2):\n",
    "        graph_add_edge(i1, i2, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print g.number_of_nodes()\n",
    "print g.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output file\n",
    "q = 'firstworldproblems' \n",
    "path = '/class/itpmssd/datasets/'\n",
    "nx.write_gexf(g, path+'%s_tweet_graph.gexf' % q)\n",
    "print path+'%s_tweet_graph.gexf' % q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hashtags with Highest Betweenness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = sorted(nx.connected_component_subgraphs(g), key=len, reverse=True)\n",
    "len(components[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betweenness = nx.betweenness_centrality(components[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k,v in sorted(betweenness.items(), key=lambda x:-x[1])[:10]:\n",
    "    print k,v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
