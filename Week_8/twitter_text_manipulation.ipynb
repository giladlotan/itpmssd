{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "# Twitter API keys go here\n",
    "CONSUMER_KEY = ''\n",
    "CONSUMER_SECRET = ''\n",
    "\n",
    "OAUTH_TOKEN = ''\n",
    "OAUTH_TOKEN_SECRET = ''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "twitter_api = twitter.Twitter(auth=auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = '#FirstWorldProblems' \n",
    "count = 100\n",
    "\n",
    "tweet_texts = []\n",
    "tweet_mentions = []\n",
    "tweet_hashtags = []\n",
    "tweet_bios = []\n",
    "ids = []\n",
    "\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "# save relevant data\n",
    "ids += [status['id'] for status in statuses]\n",
    "tweet_texts += [status['text'] for status in statuses]\n",
    "tweet_mentions += [mention['screen_name'] for status in statuses\n",
    "                       for mention in status['entities']['user_mentions']]\n",
    "tweet_hashtags += [hashtag['text'] for status in statuses\n",
    "                       for hashtag in status['entities']['hashtags']]\n",
    "tweet_bios += [status['user']['description'] for status in statuses \n",
    "                       if status['user']['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use a loop\n",
    "num_iterations = 20\n",
    "print len(set(ids))\n",
    "for i in range(num_iterations):\n",
    "    params = {a:b for a,b in [x.split('=') for x in search_results['search_metadata']['next_results'][1:].split('&')]}\n",
    "    max_id = int(params['max_id'])\n",
    "    search_results = twitter_api.search.tweets(q=q, count=count, max_id=max_id)\n",
    "    statuses = search_results['statuses']\n",
    "    \n",
    "    # save relevant data\n",
    "    tweet_texts += [status['text'] for status in statuses]\n",
    "    tweet_mentions += [mention['screen_name'] for status in statuses\n",
    "                           for mention in status['entities']['user_mentions']]\n",
    "    tweet_hashtags += [hashtag['text'] for status in statuses\n",
    "                           for hashtag in status['entities']['hashtags']]\n",
    "    tweet_bios += [status['user']['description'] for status in statuses \n",
    "                           if status['user']['description']]\n",
    "    \n",
    "    ids += [status['id'] for status in statuses]\n",
    "    print len(set(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how many more calls do we have?\n",
    "twitter_api.application.rate_limit_status()['resources']['search']['/search/tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tweet_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only represents a few different tweets\n",
    "len(set(tweet_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(tweet_texts).most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [w for tweet_text in tweet_texts for w in tweet_text.split()]\n",
    "\n",
    "print json.dumps(words[0:10], indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bio_words = [w for bio in tweet_bios for w in bio.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print top items from each array\n",
    "from collections import Counter\n",
    "\n",
    "for item in [words, bio_words, tweet_mentions, tweet_hashtags]:\n",
    "    c = Counter(item)\n",
    "    print c.most_common()[:10] # top 10\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Regular Expression\n",
    "import re\n",
    "\n",
    "stub = re.compile('[^A-Za-z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = Counter(tweet_texts)\n",
    "\n",
    "for tw, v in c.most_common(2):\n",
    "    print tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# length <=2 - unimportant?\n",
    "for tw, v in c.most_common(2):\n",
    "    print ' '.join([stub.sub('',w) for w in tw.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get rid of words that start with '@'\n",
    "\n",
    "for tw, v in c.most_common(2):\n",
    "    print ' '.join([stub.sub('',w) for w in tw.split() if not w.startswith('@')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blacklist = ['i','am','for','can','via','with','top','las','una','que','hay','todo','por','the','going','you','are','not','its','until']\n",
    "\n",
    "for tw, v in c.most_common(10):\n",
    "    cur_tweet = []\n",
    "    for w in tw.split():\n",
    "        if w.startswith('@'):\n",
    "            continue\n",
    "        cur_w = stub.sub('', w)\n",
    "        if len(cur_w)<3:\n",
    "            continue\n",
    "        if cur_w.lower() in blacklist:\n",
    "            continue\n",
    "            \n",
    "        cur_tweet.append(cur_w)\n",
    "        \n",
    "    print ' '.join(cur_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To get this working I've had to restart my ipython notebook server (which is a bit annoying -> \n",
    "# but we can do it together in class)\n",
    "# easiest is to simply restart our machines\n",
    "\n",
    "# Here's the command: \n",
    "#          /usr/bin/python /usr/local/bin/ipython notebook --notebook-dir=/class --no-browser --port=8887 &\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tw, v in c.most_common(10):\n",
    "    for w in tw.split():\n",
    "        if w.startswith('@'):\n",
    "            continue\n",
    "        cur_w = stub.sub('', w)\n",
    "        if len(cur_w)<3:\n",
    "            continue\n",
    "        if cur_w.lower() in stopwords.words('english'):\n",
    "            continue\n",
    "        if cur_w.lower() in stopwords.words('spanish'):\n",
    "            continue\n",
    "            \n",
    "        print cur_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# common words\n",
    "common_words = ['yes','way','youre']\n",
    "\n",
    "for tw, v in c.most_common(10):\n",
    "    for w in tw.split():\n",
    "        if w.startswith('http://'):\n",
    "            continue\n",
    "        cur_w = stub.sub('', w)\n",
    "        if len(cur_w)<3:\n",
    "            continue\n",
    "        if cur_w.lower() in stopwords.words('english'):\n",
    "            continue\n",
    "        if cur_w.lower() in stopwords.words('spanish'):\n",
    "            continue\n",
    "        if cur_w.lower() in common_words:\n",
    "            continue\n",
    "            \n",
    "        print cur_w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Hashtags, Mentions using a regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hashtags\n",
    "import re\n",
    "\n",
    "re_hash = re.compile(r'#[0-9a-zA-Z+_]*',re.IGNORECASE)\n",
    "hashtags = Counter()\n",
    "\n",
    "for tw in c.keys():\n",
    "    hashtags.update(re.findall(re_hash, tw))\n",
    "\n",
    "# ---> other ways to do this:\n",
    "# print re.findall(r'#(\\w+)', tw)\n",
    "# print re.findall(r'\\B#\\w+', tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtags.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mentions\n",
    "re_mention = re.compile(r'@[0-9a-zA-Z+_]*',re.IGNORECASE)\n",
    "mentions = Counter()\n",
    "\n",
    "for tw in c.keys():\n",
    "    mentions.update(re.findall(re_mention, tw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mentions.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Capitalized Phrases using a regular expression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A Counter is a container that keeps track of how many times equivalent values are added. Good tutorial here - http://pymotw.com/2/collections/counter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "phrases = Counter()\n",
    "\n",
    "comp = re.compile(r'([A-Z][a-z]+(?=\\s[A-Z])(?:\\s[A-Z][a-z]+)+)')\n",
    "for tw, v in c.items():\n",
    "    phrases.update(re.findall(comp, tw))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(                # begin capture\n",
    "  [A-Z]            # one uppercase letter  \\ First Word\n",
    "  [a-z]+           # 1+ lowercase letters  /\n",
    "  (?=\\s[A-Z])      # must have a space and uppercase letter following it\n",
    "  (?:                # non-capturing group\n",
    "    \\s               # space\n",
    "    [A-Z]            # uppercase letter   \\ Additional Word(s)\n",
    "    [a-z]+           # lowercase letter   /\n",
    "  )+              # group can be repeated (more words)\n",
    ")               #end capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrases.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a bi-partite graph using top phrases, hashtags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "limit = 5000\n",
    "\n",
    "top_phrases = [x for x,y in phrases.most_common(limit)]\n",
    "top_hashtags = [x for x,y in hashtags.most_common(limit) if len(x)>2]\n",
    "top_mentions = [x for x,y in mentions.most_common(limit) if len(x)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_hashtags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_mentions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# functions that help us construct the graph\n",
    "def graph_add_node(n, g, node_type=None):\n",
    "    try:\n",
    "        if g.has_node(n):\n",
    "            g.node[n]['weight']+=1\n",
    "        else:\n",
    "            g.add_node(n)\n",
    "            g.node[n]['label'] = n\n",
    "            g.node[n]['weight'] = 1\n",
    "            g.node[n]['type']=node_type\n",
    "    except:\n",
    "        return\n",
    "            \n",
    "def graph_add_edge(n1, n2, g):\n",
    "    if g.has_edge(n1, n2):\n",
    "        g[n1][n2]['weight']+=1\n",
    "    else:\n",
    "        g.add_edge(n1,n2)\n",
    "        g[n1][n2]['weight']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get item co-occurrence\n",
    "from itertools import combinations\n",
    "\n",
    "stub = re.compile('[^A-Za-z#@]')\n",
    "re_phrase = re.compile(r'([A-Z][a-z]+(?=\\s[A-Z])(?:\\s[A-Z][a-z]+)+)')\n",
    "re_mention_hashtag = re.compile(r'[@|#][0-9a-zA-Z+_]*',re.IGNORECASE)\n",
    "\n",
    "g = nx.Graph()\n",
    "\n",
    "for t,v in c.most_common(30000):\n",
    "    cur_items = set()\n",
    "    \n",
    "    for phrase in re.findall(re_phrase, t):\n",
    "        cur_items.add(phrase)\n",
    "        graph_add_node(phrase, g, node_type='phrase')\n",
    "        \n",
    "    for men_hash in re.findall(re_mention_hashtag, t):\n",
    "        cur_items.add(men_hash)\n",
    "        if men_hash.startswith('#'):\n",
    "            graph_add_node(men_hash, g, node_type='hashtag')\n",
    "        elif men_hash.startswith('@'):\n",
    "            graph_add_node(men_hash, g, node_type='mention')\n",
    "        \n",
    "    # add edges\n",
    "    for i1, i2 in combinations(cur_items, 2):\n",
    "        graph_add_edge(i1, i2, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print g.number_of_nodes()\n",
    "print g.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output file\n",
    "q = 'firstworldproblems' \n",
    "path = '/class/itpmssd/datasets/'\n",
    "nx.write_gexf(g, path+'%s_tweet_graph.gexf' % q)\n",
    "print path+'%s_tweet_graph.gexf' % q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hashtags with Highest Betweenness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = sorted(nx.connected_component_subgraphs(g), key=len, reverse=True)\n",
    "len(components[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betweenness = nx.betweenness_centrality(components[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k,v in sorted(betweenness.items(), key=lambda x:-x[1])[:10]:\n",
    "    print k,v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
