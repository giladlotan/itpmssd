{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From CSV File: Stop and Frisk / NYPD"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's try to grab data from a publicly available CSV file - the NYPD's Stop and Frisk dataset:\n",
    "- http://www.nyclu.org/files/stopandfrisk/Stop-and-Frisk-2012.zip\n",
    "\n",
    "Here's a description of what the dataset includes:\n",
    "- http://www.nyclu.org/files/SQF_Codebook.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's grab the csv file ('wget'), unzip it, and change its name to something that doesn't include spaces.\n",
    "# SSH into your remote machine and run the following commands:\n",
    "\n",
    "''' \n",
    "    cd /class/itpmssd/datasets\n",
    "    wget http://www.nyclu.org/files/stopandfrisk/Stop-and-Frisk-2012.zip\n",
    "    unzip Stop-and-Frisk-2012.zip\n",
    "    mv SQF\\ 2012.csv sf2012.csv\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Now let's explore the dataset using the command line first"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. How many rows in our dataset:\n",
    "    - \"cat sf2012.csv | wc -l\"\n",
    "\n",
    "\n",
    "2. What are the column names: (effectively the description of all variables)\n",
    "    - \"head -1 sf2012.csv\"\n",
    "\n",
    "(this dataset is WAY too big to fully load into our tiny little machine)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. 532k rows of data\n",
    "\n",
    "2. year,pct,ser_num,datestop,timestop,city,sex,race,dob,age,height,weight,haircolr,eyecolor,build,othfeatr,frisked,searched,contrabn,pistol,riflshot,asltweap,knifcuti,machgun,othrweap,arstmade,arstoffn,sumissue,sumoffen,crimsusp,detailcm,perobs,perstop,pf_hands,pf_wall,pf_grnd,pf_drwep,pf_ptwep,pf_baton,pf_hcuff,pf_pepsp,pf_other,cs_objcs,cs_descr,cs_casng,cs_lkout,cs_cloth,cs_drgtr,cs_furtv,cs_vcrim,cs_bulge,cs_other,rf_vcrim,rf_othsw,rf_attir,rf_vcact,rf_rfcmp,rf_verbl,rf_knowl,rf_furt,rf_bulg,sb_hdobj,sb_outln,sb_admis,sb_other,ac_proxm,ac_evasv,ac_assoc,ac_cgdir,ac_incid,ac_time,ac_stsnd,ac_other,forceuse,inout,trhsloc,premname,addrnum,stname,stinter,crossst,addrpct,sector,beat,post,xcoord,ycoord,typeofid,othpers,explnstp,repcmd,revcmd,offunif,offverb,officrid,offshld,ac_rept,ac_inves,radio,recstat,linecm\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's start by creating a smaller file -> take the first 100,000 rows from the original file:\n",
    "\n",
    "\"head -100000 sf2012.csv > sf2012_sm.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we can open the file\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "path = '/class/itpmssd/datasets/sf2012_sm.csv'\n",
    "df = pd.read_csv(path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# different ways to look at the data (since it is way too large to simply print)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[900:905]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The more you work with this dataset, and generally data writ large, the more you'll realize how messy it is. There will be empty fields, fields that don't really make sense, and they all will mess up your analysis process at some point. We have be constantly on the lookout for issues with our data, throughout our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# translate the messy date string into something cleaner -> a datetime structure\n",
    "# (M)MDDYYYY\n",
    "\n",
    "def parse_hour(timestop):\n",
    "    minute = timestop % 100\n",
    "    hour = timestop / 100\n",
    "    return hour,minute\n",
    "\n",
    "def parse_date(datestop):\n",
    "    month = int(str(datestop)[:-6])\n",
    "    day = int(str(datestop)[-6:-4])\n",
    "    year = int(str(datestop)[-4:])\n",
    "    return year, month, day\n",
    "\n",
    "def make_datetime(datestop, timestop):\n",
    "    year, month, day = parse_date(datestop)\n",
    "    hour, minute = parse_hour(timestop)\n",
    "    return datetime.datetime(year, month, day, hour)\n",
    "\n",
    "def make_date(datestop):\n",
    "    year, month, day = parse_date(datestop)\n",
    "    return datetime.datetime(year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print parse_date(df.ix[0].datestop)\n",
    "print parse_hour(df.ix[0].timestop)\n",
    "print make_datetime(df.ix[0].datestop, df.ix[0].timestop)\n",
    "print make_date(df.ix[0].datestop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now lets add a parsed datetime row in our data frame - by applying the function that we've just built\n",
    "# this will give us a 'dt' field we can use to group by\n",
    "\n",
    "df['dt']=df[['datestop','timestop']].apply(lambda x: make_datetime(x['datestop'], x['timestop']), axis=1)\n",
    "df['d']=df[['datestop']].apply(lambda x: make_date(x['datestop']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping: split-apply-combine"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By \"group by\" we are referring to a process involving one or more of the following steps:\n",
    "\n",
    "- Split: the data into groups based on some criteria\n",
    "- Apply: a function to each group independently\n",
    "- Combine: the results into a new data structure\n",
    "\n",
    "In the case of our dataset, we'd like to group our data by day+hour and display the number of reported incidents over time, using hourly bins. What we'll need to do in order to make this happen:\n",
    "\n",
    "- Split the data by its index (the dt column)\n",
    "- Count the number of items in each group (number of incidents reported each day)\n",
    "- And bring this information back together into a new Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grouping - counting the number of reported incidents per hour (can also do grouped.count())\n",
    "\n",
    "df.groupby(df.dt).size()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's make a plot - that shows trends over time\n",
    "%pylab inline\n",
    "\n",
    "df.groupby(df.dt).size().plot(figsize=(20,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can easily specify a smaller date range\n",
    "df.groupby(df.dt).size()[:'2012-02-16'].plot(figsize=(20,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's examine age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apparently immortality is imminent\n",
    "\n",
    "sns.boxplot(df.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DRAAAAATS - more weirdnesses in the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# argmax prints the row # for the maximum value in a given column\n",
    "print 'maximum age index number:',df.age.argmax(),'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ix[] -> is a way to access values given a specific row\n",
    "print 'maximum age row:',df.ix[1021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How bad is it? How many rows in our data have weird age figures?\n",
    "MAX_AGE = 100\n",
    "\n",
    "print 'number of problematic rows:',len(df[[a>MAX_AGE for a in df.age]])\n",
    "\n",
    "# 0.2% of our data is problematic\n",
    "len(df[[a>MAX_AGE for a in df.age]])/float(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter out rows with faulty age info\n",
    "df = df[[x<MAX_AGE for x in df.age]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try this again\n",
    "sns.boxplot(df.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can always get rid of unnecessary columns - makes DataFrames easier to handle\n",
    "\n",
    "wanted_columns = ['dt','d','age','sex','race','height','weight','build','frisked']\n",
    "\n",
    "sm_df = df[wanted_columns]\n",
    "sm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's take a look at Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "race_labels = {\n",
    "    1:'black', \n",
    "    2:'black_Hispanic', \n",
    "    3:'white_Hispanic', \n",
    "    4:'white', \n",
    "    5:'Asian_Pacific_Islander', \n",
    "    6:'Am_Indian_Native_Alaskan'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sm_df.race[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can use our handy dictionary to apply the labels\n",
    "[race_labels[x] for x in sm_df.race[10:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Handy way to count values in a column\n",
    "\n",
    "sm_df.race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# histogram: show entries by race\n",
    "sm_df.race.hist(figsize=(12,6))\n",
    "ylabel('number of incidents')\n",
    "xlabel('race')\n",
    "title('incidents by race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = sm_df.groupby('race')\n",
    "black = grouped.get_group(1)\n",
    "white_hispanic = grouped.get_group(3)\n",
    "white = grouped.get_group(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "black.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "grouped.get_group(1).hist(bins=30,figsize=(14,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot frisks broken down by race over time\n",
    "black.groupby('dt').frisked.size()[:'2012-02-16'].plot(figsize=(20,6), label='black')\n",
    "white_hispanic.groupby('dt').frisked.size()[:'2012-02-16'].plot(figsize=(20,6), label='white hispanic')\n",
    "white.groupby('dt').frisked.size()[:'2012-02-16'].plot(figsize=(20,6), label='white')\n",
    "legend()\n",
    "title('frisked over time, by race')\n",
    "ylabel('number of frisks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# multi-bar charts (note: they are overlayed on top of each other)\n",
    "\n",
    "black.groupby('d').size()[:'2012-02-16'].plot(kind=\"bar\", figsize=(20,6), label='black')\n",
    "white_hispanic.groupby('d').size()[:'2012-02-16'].plot(kind=\"bar\", color='red', figsize=(20,6), label='white_hispanic')\n",
    "white.groupby('d').size()[:'2012-02-16'].plot(kind=\"bar\", color='orange', figsize=(20,6), label='white')\n",
    "xticks(rotation=45)\n",
    "legend()\n",
    "title('daily incidents, by race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stacked bar chart\n",
    "\n",
    "sm_df.groupby(['d','race']).size()[:'2012-02-16'].unstack('race').plot(kind=\"bar\", figsize=(20,6), stacked=True)\n",
    "\n",
    "xticks(rotation=45)\n",
    "legend()\n",
    "title('daily incidents, by race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# frisked vs. not frisked - from the population of incidents involving black people\n",
    "\n",
    "black.groupby(['d','frisked']).size()[:'2012-02-16'].unstack('frisked').plot(kind=\"bar\", figsize=(18,6))\n",
    "title('daily frisks, for race=black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sm_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12, 10))\n",
    "sns.corrplot(sm_df, annot=False, diag_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the small DataFrame (for when our system crashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the data for later\n",
    "import pickle\n",
    "\n",
    "f = open('/class/itpmssd/Week_2/sm_df.p','wb')\n",
    "pickle.dump(sm_df,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the data from pickled file\n",
    "\n",
    "pkl_file = open('/class/itpmssd/Week_2/sm_df.p','rb')\n",
    "sm_df = pickle.load(pkl_file)\n",
    "\n",
    "# ... aaand VOILA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Smaller Stop-and-Frisk File/DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Smaller File -> so that we can look at longer term trends\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "columns = ['datestop','timestop','sex','race','age','height','weight','build','frisked']\n",
    "wanted_indices = set([3,4,6,7,9,10,11,14,16])\n",
    "\n",
    "columns = {\n",
    "    3:'datestop',\n",
    "    4:'timestop',\n",
    "    6:'sex',\n",
    "    7:'race',\n",
    "    9:'age',\n",
    "    10:'height',\n",
    "    11:'weight',\n",
    "    14:'build',\n",
    "    16:'frisked'\n",
    "}\n",
    "\n",
    "small_dict = {c:[] for c in columns.values()}\n",
    "\n",
    "cnt = 0\n",
    "with open(\"/class/itpmssd/datasets/sf2012.csv\",'rb') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        if cnt==0:\n",
    "            # dismiss the first row of the CSV file -> column headers\n",
    "            cnt += 1\n",
    "            continue\n",
    "        \n",
    "        # parse the date from the given string\n",
    "        cur_date = calc_datetime(row[3])\n",
    "        \n",
    "        for k,c in columns.items():\n",
    "            if c=='datestop':\n",
    "                small_dict[c].append(cur_date)\n",
    "            else:\n",
    "                if row[k]!='':\n",
    "                    # add relevant value to the dictionary\n",
    "                    small_dict[c].append(int(row[k]))\n",
    "                else:\n",
    "                    # add zeros where there's no data -> generally messes up our distribution estimation\n",
    "                    small_dict[c].append(0)\n",
    "        \n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sm_df = pd.DataFrame.from_dict(small_dict)\n",
    "sm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Reads and References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precinct or Prejudice? understanding racial disparities in New York City's stop-and-frisk policy: https://5harad.com/papers/frisky.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
