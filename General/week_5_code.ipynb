{
 "metadata": {
  "name": "week_5_code.ipynb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Twitter Text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import twitter\n",
      "\n",
      "# Twitter API keys go here\n",
      "CONSUMER_KEY = ''\n",
      "CONSUMER_SECRET = ''\n",
      "\n",
      "OAUTH_TOKEN = ''\n",
      "OAUTH_TOKEN_SECRET = ''\n",
      "\n",
      "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
      "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
      "\n",
      "twitter_api = twitter.Twitter(auth=auth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "q = 'OccupyCentral' \n",
      "count = 100\n",
      "\n",
      "tweet_texts = []\n",
      "tweet_mentions = []\n",
      "tweet_hashtags = []\n",
      "tweet_bios = []\n",
      "ids = []\n",
      "\n",
      "search_results = twitter_api.search.tweets(q=q, count=count)\n",
      "statuses = search_results['statuses']\n",
      "\n",
      "# save relevant data\n",
      "ids += [status['id'] for status in statuses]\n",
      "tweet_texts += [status['text'] for status in statuses]\n",
      "tweet_mentions += [mention['screen_name'] for status in statuses\n",
      "                       for mention in status['entities']['user_mentions']]\n",
      "tweet_hashtags += [hashtag['text'] for status in statuses\n",
      "                       for hashtag in status['entities']['hashtags']]\n",
      "tweet_bios += [status['user']['description'] for status in statuses \n",
      "                       if status['user']['description']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use a loop\n",
      "num_iterations = 4\n",
      "print len(set(ids))\n",
      "print ''\n",
      "for i in range(num_iterations):\n",
      "    params = {a:b for a,b in [x.split('=') for x in search_results['search_metadata']['next_results'][1:].split('&')]}\n",
      "    max_id = int(params['max_id'])\n",
      "    search_results = twitter_api.search.tweets(q=q, count=count, max_id=max_id)\n",
      "    statuses = search_results['statuses']\n",
      "    \n",
      "    # save relevant data\n",
      "    tweet_texts += [status['text'] for status in statuses]\n",
      "    tweet_mentions += [mention['screen_name'] for status in statuses\n",
      "                           for mention in status['entities']['user_mentions']]\n",
      "    tweet_hashtags += [hashtag['text'] for status in statuses\n",
      "                           for hashtag in status['entities']['hashtags']]\n",
      "    tweet_bios += [status['user']['description'] for status in statuses \n",
      "                           if status['user']['description']]\n",
      "    \n",
      "    ids += [status['id'] for status in statuses]\n",
      "    print len(set(ids))\n",
      "    print ''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100\n",
        "\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# how many more calls do we have?\n",
      "twitter_api.application.rate_limit_status()['resources']['search']['/search/tweets']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(tweet_texts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# only represents a few different tweets\n",
      "len(set(tweet_texts))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Counter(tweet_texts).most_common()[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "# Compute a collection of all words from all tweets\n",
      "words = [ w for tweet_text in tweet_texts for w in tweet_text.split() ]\n",
      "\n",
      "print json.dumps(words[0:10], indent=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bio_words = [w for bio in tweet_bios for w in bio.split() ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "for item in [words, bio_words, tweet_mentions, tweet_hashtags]:\n",
      "    c = Counter(item)\n",
      "    print c.most_common()[:10] # top 10\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "...OR... Load Tweet data from a pickled collection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load a picked Counter object filled with Tweets\n",
      "import pickle\n",
      "\n",
      "path = '/Users/giladlotan/Documents/ITP Course/code/Week5/'\n",
      "c = pickle.load(open(path+'occupycentral_tweets.p','rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(c.keys()),'different tweets'\n",
      "print ''\n",
      "print c.most_common(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Clean up our text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# regex?\n",
      "import re\n",
      "\n",
      "stub = re.compile('[^A-Za-z]')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for tw, v in c.most_common(1):\n",
      "    print tw\n",
      "    print tw.split()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# length <=2 - unimportant?\n",
      "for tw, v in c.most_common(1):\n",
      "    for w in tw.split():\n",
      "        # substitute any character that's not covered in our stub with ''\n",
      "        cur_w = stub.sub('', w)\n",
      "        if len(cur_w)>2:\n",
      "            print cur_w\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get rid of urls\n",
      "for tw, v in c.most_common(10):\n",
      "    for w in tw.split():\n",
      "        if w.startswith('http://'):\n",
      "            continue\n",
      "        cur_w = stub.sub('', w)\n",
      "        if len(cur_w)<3:\n",
      "            continue\n",
      "        print cur_w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Stopwords"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blacklist = ['i','am','for','can','via','with','top','las','una','que','hay','todo','por','the','going','you','are','not','its','until']\n",
      "\n",
      "for tw, v in c.most_common(10):\n",
      "    for w in tw.split():\n",
      "        if w.startswith('http://'):\n",
      "            continue\n",
      "        cur_w = stub.sub('', w)\n",
      "        if len(cur_w)<3:\n",
      "            continue\n",
      "        if cur_w.lower() in blacklist:\n",
      "            continue\n",
      "            \n",
      "        print cur_w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pip install nltk\n",
      "# goto command line and type: \n",
      "# import nltk\n",
      "# nltk.download()\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stopwords.words('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stopwords.words('spanish')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for tw, v in c.most_common(10):\n",
      "    for w in tw.split():\n",
      "        if w.startswith('http://'):\n",
      "            continue\n",
      "        cur_w = stub.sub('', w)\n",
      "        if len(cur_w)<3:\n",
      "            continue\n",
      "        if cur_w.lower() in stopwords.words('english'):\n",
      "            continue\n",
      "        if cur_w.lower() in stopwords.words('spanish'):\n",
      "            continue\n",
      "            \n",
      "        print cur_w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# common words\n",
      "common_words = ['via','top','new']\n",
      "\n",
      "for tw, v in c.most_common(10):\n",
      "    for w in tw.split():\n",
      "        if w.startswith('http://'):\n",
      "            continue\n",
      "        cur_w = stub.sub('', w)\n",
      "        if len(cur_w)<3:\n",
      "            continue\n",
      "        if cur_w.lower() in stopwords.words('english'):\n",
      "            continue\n",
      "        if cur_w.lower() in stopwords.words('spanish'):\n",
      "            continue\n",
      "        if cur_w.lower() in common_words:\n",
      "            continue\n",
      "            \n",
      "        print cur_w\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Find Hashtags, Mentions using a regular expression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Hashtags\n",
      "import re\n",
      "\n",
      "re_hash = re.compile(r'#[0-9a-zA-Z+_]*',re.IGNORECASE)\n",
      "hashtags = Counter()\n",
      "\n",
      "for tw in c.keys():\n",
      "    hashtags.update(re.findall(re_hash, tw))\n",
      "\n",
      "# ---> other ways to do this:\n",
      "# print re.findall(r'#(\\w+)', tw)\n",
      "# print re.findall(r'\\B#\\w+', tw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hashtags.most_common(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Mentions\n",
      "re_mention = re.compile(r'@[0-9a-zA-Z+_]*',re.IGNORECASE)\n",
      "mentions = Counter()\n",
      "\n",
      "for tw in c.keys():\n",
      "    mentions.update(re.findall(re_mention, tw))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mentions.most_common(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Find Capitalized Phrases using a regular expression"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "A Counter is a container that keeps track of how many times equivalent values are added. Good tutorial here - http://pymotw.com/2/collections/counter.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "phrases = Counter()\n",
      "\n",
      "comp = re.compile(r'([A-Z][a-z]+(?=\\s[A-Z])(?:\\s[A-Z][a-z]+)+)')\n",
      "for tw, v in c.items():\n",
      "    phrases.update(re.findall(comp, tw))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "(                # begin capture\n",
      "  [A-Z]            # one uppercase letter  \\ First Word\n",
      "  [a-z]+           # 1+ lowercase letters  /\n",
      "  (?=\\s[A-Z])      # must have a space and uppercase letter following it\n",
      "  (?:                # non-capturing group\n",
      "    \\s               # space\n",
      "    [A-Z]            # uppercase letter   \\ Additional Word(s)\n",
      "    [a-z]+           # lowercase letter   /\n",
      "  )+              # group can be repeated (more words)\n",
      ")               #end capture"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "phrases.most_common(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(phrases)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "make a bi-partite graph using top phrases, hashtags and mentions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "limit = 5000\n",
      "\n",
      "top_phrases = [x for x,y in phrases.most_common(limit)]\n",
      "top_hashtags = [x for x,y in hashtags.most_common(limit) if len(x)>2]\n",
      "top_mentions = [x for x,y in mentions.most_common(limit) if len(x)>2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top_hashtags[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top_mentions[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Generate Graph"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx as nx\n",
      "\n",
      "# functions that help us construct the graph\n",
      "def graph_add_node(n, g, node_type=None):\n",
      "    try:\n",
      "        if g.has_node(n):\n",
      "            g.node[n]['weight']+=1\n",
      "        else:\n",
      "            g.add_node(n)\n",
      "            g.node[n]['label'] = n\n",
      "            g.node[n]['weight'] = 1\n",
      "            g.node[n]['type']=node_type\n",
      "    except:\n",
      "        return\n",
      "            \n",
      "def graph_add_edge(n1, n2, g):\n",
      "    if g.has_edge(n1, n2):\n",
      "        g[n1][n2]['weight']+=1\n",
      "    else:\n",
      "        g.add_edge(n1,n2)\n",
      "        g[n1][n2]['weight']=1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get item co-occurrence\n",
      "from itertools import combinations\n",
      "\n",
      "stub = re.compile('[^A-Za-z#@]')\n",
      "re_phrase = re.compile(r'([A-Z][a-z]+(?=\\s[A-Z])(?:\\s[A-Z][a-z]+)+)')\n",
      "re_mention_hashtag = re.compile(r'[@|#][0-9a-zA-Z+_]*',re.IGNORECASE)\n",
      "\n",
      "g = nx.Graph()\n",
      "\n",
      "for t,v in c.most_common(30000):\n",
      "    cur_items = set()\n",
      "    \n",
      "    for phrase in re.findall(re_phrase, t):\n",
      "        cur_items.add(phrase)\n",
      "        graph_add_node(phrase, g, node_type='phrase')\n",
      "        \n",
      "    for men_hash in re.findall(re_mention_hashtag, t):\n",
      "        cur_items.add(men_hash)\n",
      "        if men_hash.startswith('#'):\n",
      "            graph_add_node(men_hash, g, node_type='hashtag')\n",
      "        elif men_hash.startswith('@'):\n",
      "            graph_add_node(men_hash, g, node_type='mention')\n",
      "        \n",
      "    # add edges\n",
      "    for i1, i2 in combinations(cur_items, 2):\n",
      "        graph_add_edge(i1, i2, g)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print g.number_of_nodes()\n",
      "print g.number_of_edges()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output file\n",
      "q = 'OccupyCentral' \n",
      "path = '/Users/giladlotan/Documents/ITP Course/code/Week5/'\n",
      "nx.write_gexf(g, path+'%s_tweet_graph.gexf' % q)\n",
      "print path+'%s_tweet_graph.gexf' % q"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "components = sorted(nx.connected_component_subgraphs(g), key=len, reverse=True)\n",
      "len(components[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "betweenness = nx.betweenness_centrality(components[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}